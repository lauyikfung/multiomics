{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training part of TNBC dataset, using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import pandas as pd\n",
    "from database.entity_process import *\n",
    "start_time = time.time()\n",
    "subtype_to_num = {'BLIS':0, 'IM':1, 'LAR':2, 'MES':3}\n",
    "def processed_TNBC_data():\n",
    "  data_all = pd.read_excel('TNBC_new/all_module_complete.xlsx')\n",
    "  name_list = list(data_all.keys())[3:]\n",
    "  all_data = [[subtype_to_num[data_all.iloc[i, 2]], data_all.iloc[i, 3:]] for i in range(258)]\n",
    "  return all_data, name_list\n",
    "all_data, name_list = processed_TNBC_data()\n",
    "random.seed(2022)\n",
    "random.shuffle(all_data)\n",
    "\n",
    "pro_list = pd.Index(list(name_list))\n",
    "all_data = [[i[0], i[1][:]] for i in all_data]\n",
    "all_data_pro_tv = all_data[:200]\n",
    "tests = all_data[200:]\n",
    "\n",
    "\n",
    "import csv\n",
    "with open('./TNBC_new/temp_tv.csv','w') as file:\n",
    "    fieldnames = ['label']\n",
    "    fieldnames.extend(pro_list)\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for item in all_data_pro_tv:\n",
    "        dat = {'label': item[0]}\n",
    "        for i in range(len(item[1])):\n",
    "            dat[pro_list[i]] = item[1][i]\n",
    "        writer.writerow(dat)\n",
    "with open('./TNBC_new/temp_test.csv','w') as file:\n",
    "    fieldnames = ['label']\n",
    "    fieldnames.extend(pro_list)\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for item in tests:\n",
    "        dat = {'label': item[0]}\n",
    "        for i in range(len(item[1])):\n",
    "            dat[pro_list[i]] = item[1][i]\n",
    "        writer.writerow(dat)\n",
    "train = pd.read_csv(\"./TNBC_new/temp_tv.csv\")\n",
    "tests = pd.read_csv(\"./TNBC_new/temp_test.csv\")\n",
    "test_label=tests.iloc[:,0]\n",
    "params={\n",
    "    'booster':'gbtree',\n",
    "    'objective':'multi:softmax',\n",
    "    'num_class':4,\n",
    "    'gamma':0.1,\n",
    "    'max_depth':12,\n",
    "    'lambda':0,\n",
    "    'subsample':0.7,\n",
    "    'colsample_bytree':0.7,\n",
    "    'min_child_weight':3,\n",
    "    'silent':0,\n",
    "    'eta':0.007,\n",
    "    'seed':1000,\n",
    "    'nthread':7,\n",
    "}\n",
    "plst = list(params.items())\n",
    "num_rounds = 50000\n",
    "train_xy, val = train_test_split(train,test_size = 0.3,random_state=1)\n",
    "y = train_xy.label\n",
    "X = train_xy.drop(['label'],axis = 1)\n",
    "val_y = val.label\n",
    "val_X = val.drop(['label'],axis = 1)\n",
    "xgb_val = xgb.DMatrix(val_X, label=val_y)\n",
    "xgb_train = xgb.DMatrix(X, label=y)\n",
    "y1 = tests.label\n",
    "x1 = tests.drop(['label'],axis = 1)\n",
    "xgb_test = xgb.DMatrix(x1, label=y1)\n",
    "watchlist = [(xgb_train,'train'),(xgb_val, 'val')]\n",
    "model = xgb.train(plst,xgb_train,num_rounds,watchlist,early_stopping_rounds=200)\n",
    "model.save_model('./TNBC_new/xgb.model')\n",
    "print('best best_ntree_limit',model.best_ntree_limit)\n",
    "preds=model.predict(xgb_test,ntree_limit=model.best_ntree_limit)\n",
    "accuracy_test=accuracy_score(preds,test_label)\n",
    "print(f\"the accuracy is {accuracy_test}.\")\n",
    "np.savetxt('./TNBC_new/xgb_submission.csv',np.c_[range(1,len(tests)+1),preds, test_label],delimiter=',',header='ImageId,Label,GroundTruth',comments='',fmt='%d')\n",
    "cost_time=time.time()-start_time\n",
    "print('success!','\\n','cost time:',cost_time,'(s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The utilization of the trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following part should repeat twice! to generate fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import pandas as pd\n",
    "from database.entity_process import *\n",
    "start_time = time.time()\n",
    "subtype_to_num = {'BLIS':0, 'IM':1, 'LAR':2, 'MES':3}\n",
    "def processed_TNBC_data():\n",
    "  data_all = pd.read_excel('TNBC_new/all_module_complete.xlsx')\n",
    "  name_list = list(data_all.keys())[3:]\n",
    "  all_data = [[subtype_to_num[data_all.iloc[i, 2]], data_all.iloc[i, 3:]] for i in range(258)]\n",
    "  return all_data, name_list\n",
    "all_data, name_list = processed_TNBC_data()\n",
    "random.seed(2022)\n",
    "random.shuffle(all_data)\n",
    "\n",
    "pro_list = pd.Index(list(name_list))\n",
    "all_data = [[i[0], i[1][:]] for i in all_data]\n",
    "all_data_pro_tv = all_data[:200]\n",
    "tests = all_data[200:]\n",
    "features2 = [i.replace(\" \", \"-\") for i in name_list]\n",
    "import csv\n",
    "with open('./TNBC_new/temp_tv.csv','w') as file:\n",
    "    fieldnames = ['label']\n",
    "    fieldnames.extend(pro_list)\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for item in all_data_pro_tv:\n",
    "        dat = {'label': item[0]}\n",
    "        for i in range(len(item[1])):\n",
    "            dat[pro_list[i]] = item[1][i]\n",
    "        writer.writerow(dat)\n",
    "with open('./TNBC_new/temp_test.csv','w') as file:\n",
    "    fieldnames = ['label']\n",
    "    fieldnames.extend(pro_list)\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for item in tests:\n",
    "        dat = {'label': item[0]}\n",
    "        for i in range(len(item[1])):\n",
    "            dat[pro_list[i]] = item[1][i]\n",
    "        writer.writerow(dat)\n",
    "train = pd.read_csv(\"./TNBC_new/temp_tv.csv\")\n",
    "tests = pd.read_csv(\"./TNBC_new/temp_test.csv\")\n",
    "test_label=tests.iloc[:,0]\n",
    "params={\n",
    "    'booster':'gbtree',\n",
    "    'objective':'multi:softmax',\n",
    "    'num_class':4,\n",
    "    'gamma':0.1,\n",
    "    'max_depth':12,\n",
    "    'lambda':0,\n",
    "    'subsample':0.7,\n",
    "    'colsample_bytree':0.7,\n",
    "    'min_child_weight':3,\n",
    "    'silent':0,\n",
    "    'eta':0.007,\n",
    "    'seed':1000,\n",
    "    'nthread':7,\n",
    "}\n",
    "plst = list(params.items())\n",
    "num_rounds = 50000\n",
    "train_xy, val = train_test_split(train,test_size = 0.3,random_state=1)\n",
    "y = train_xy.label\n",
    "X = train_xy.drop(['label'],axis = 1)\n",
    "val_y = val.label\n",
    "val_X = val.drop(['label'],axis = 1)\n",
    "xgb_val = xgb.DMatrix(val_X, label=val_y)\n",
    "xgb_train = xgb.DMatrix(X, label=y)\n",
    "y1 = tests.label\n",
    "x1 = tests.drop(['label'],axis = 1)\n",
    "xgb_test = xgb.DMatrix(x1, label=y1)\n",
    "watchlist = [(xgb_train,'train'),(xgb_val, 'val')]\n",
    "f1 = open(\"./TNBC_new/xgb.model.fmap\", \"w\") \n",
    "for i, feat in enumerate(features2):      \n",
    "    f1.write('{0}\\t{1}\\tq\\n'.format(i, feat)) \n",
    "import xgboost as xgb\n",
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "# load model.\n",
    "bst = xgb.Booster()\n",
    "bst.load_model('./TNBC_new/xgb.model')\n",
    "model=bst\n",
    "# # plot\n",
    "plot_tree(bst, fmap='./TNBC_new/xgb.model.fmap',num_trees=0, rankdir='UT', ax=None)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The importance bar chart of XGBoost importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 3\n",
    "graph = xgb.to_graphviz(bst, num_trees=NUM, **{'size': str(10)})\n",
    "graph.render(filename='./TNBC/xgb_{}.dot'.format(NUM))\n",
    "xgb.plot_importance(model, max_num_features=20, fmap='./TNBC_new/xgb.model.fmap')\n",
    "model.dump_model(\"./TNBC_new/model.txt\")\n",
    "plt.savefig(\"./TNBC_new/xgb_importance.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of \".graphml\" file according to some constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "imp = model.get_score(\n",
    "            importance_type='gain', fmap='./TNBC_new/xgb.model.fmap')#weight/gain/cover\n",
    "kk = list(imp.values())\n",
    "import json\n",
    "with open(\"TNBC_new/compound_reaction_complete.json\", \"r\") as f:\n",
    "  s = json.load(f)\n",
    "names = [[i[2], i[0]] for i in s]\n",
    "with open(\"TNBC_new/transcript_reaction_complete.json\", \"r\") as f:\n",
    "  s = json.load(f)\n",
    "names += [[i[0], i[1]] for i in s]\n",
    "names = [[i[0].replace(\"-\", \" \"), i[1]] for i in names]\n",
    "gene_ortholog = [i[2] for i in s]\n",
    "name_dict = {}\n",
    "for item in names:\n",
    "  name_dict[item[0]] = item[1]\n",
    "\n",
    "score_dict = {}\n",
    "\n",
    "for item in imp.keys():\n",
    "  item_2 = item.replace(\"-\",\" \")\n",
    "  score_dict[name_dict[item_2]] = imp[item]\n",
    "from neo4j import GraphDatabase\n",
    "from py2neo import Node, Relationship, Path\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from py2neo import Graph as pGraph\n",
    "import py2neo\n",
    "from matplotlib import pyplot as plt\n",
    "from igraph import Graph as iGraph\n",
    "import igraph\n",
    "import networkx as nx\n",
    "import time\n",
    "import sys\n",
    "THRESHOLD = 0.5\n",
    "def graph_from_cypher(data):\n",
    "    \"\"\"Constructs a networkx graph from the results of a neo4j cypher query.\n",
    "    Example of use:\n",
    "    >>> result = session.run(query)\n",
    "    >>> G = graph_from_cypher(result.data())\n",
    "\n",
    "    Nodes have fields 'labels' (frozenset) and 'properties' (dicts). Node IDs correspond to the neo4j graph.\n",
    "    Edges have fields 'type_' (string) denoting the type of relation, and 'properties' (dict).\"\"\"\n",
    "\n",
    "    G = nx.MultiDiGraph()\n",
    "    def add_node(node):\n",
    "        # Adds node id it hasn't already been added\n",
    "        u = node.identity\n",
    "        if G.has_node(u):\n",
    "            return\n",
    "        G.add_node(u, labels=node._labels, properties=dict(node))\n",
    "\n",
    "    def add_edge(relation):\n",
    "        # Adds edge if it hasn't already been added.\n",
    "        # Make sure the nodes at both ends are created\n",
    "        for node in (relation.start_node, relation.end_node):\n",
    "            add_node(node)\n",
    "        # Check if edge already exists\n",
    "        rel_type = str(type(relation))\n",
    "        u = relation.start_node.identity\n",
    "        v = relation.end_node.identity\n",
    "        eid = relation.identity\n",
    "        if G.has_edge(u, v, key=eid):\n",
    "            return\n",
    "        # If not, create it\n",
    "        \n",
    "        G.add_edge(u, v, key=eid, type_=rel_type[rel_type.find(\"data.\")+5:-2], properties=dict(relation))\n",
    "\n",
    "    def add_path(entry):\n",
    "        for rel in entry.relationships:\n",
    "            add_edge(rel)\n",
    "\n",
    "    for d in data:\n",
    "        for entry in d.values():\n",
    "            # Parse node\n",
    "            if isinstance(entry, Node):\n",
    "                add_node(entry)\n",
    "\n",
    "            # Parse link\n",
    "            elif isinstance(entry, Relationship):\n",
    "                add_edge(entry)\n",
    "            elif isinstance(entry, Path):\n",
    "                add_path(entry)\n",
    "            else:\n",
    "                raise TypeError(\"Unrecognized object\")\n",
    "    return G\n",
    "start_time = time.time()\n",
    "neo4j = pGraph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"libiao\"))\n",
    "all_available_kid = list(score_dict.keys())\n",
    "\n",
    "dats = neo4j.run(\"match (n1:compound)-[p:substrate]-(n2:reaction)-[q:product]-(n3:compound),\\\n",
    "    (n2)-[r:enzyme]-(n4:ortholog)-[s:gene_expression]-(n5:gene) where n1.TNBC_valid = 1 and \\\n",
    "        n3.TNBC_valid = 1 and n5.kid in {} return *;\".format(all_available_kid))\n",
    "\n",
    "# dats = neo4j.run(\"match (n1:compound)-[p:substrate]-(n2:reaction)-[q:product]-(n3:compound),\\\n",
    "#     (n6:pathway)-[t:in_pathway]-(n2)-[r:enzyme]-(n4:ortholog)-[s:gene_expression]-(n5:gene) where n1.TNBC_valid = 1 and \\\n",
    "#         n3.TNBC_valid = 1 and n5.kid in {} and n6.kid in ['path:hsa00230'] return *;\".format(all_available_kid))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Data downloaded\")\n",
    "print(str(round((end_time-start_time)/60, 2))+ \"min\")\n",
    "G = graph_from_cypher(dats.data())\n",
    "print(\"Graph converted\")\n",
    "end_time = time.time()\n",
    "print(str(round((end_time-start_time)/60, 2))+ \"min\")\n",
    "import copy\n",
    "gg = iGraph.from_networkx(G)\n",
    "# EDGE_TYPE = ['expression', 'gene_expression', 'repression', 'activation', 'inhibition', 'substrate', 'product', 'enzyme', 'in_module', 'in_pathway']\n",
    "EDGE_TYPE = ['expression', 'gene_expression', 'repression', 'activation', 'inhibition', 'substrate', 'product', 'enzyme', 'in_module',]\n",
    "for edge in gg.es:\n",
    "    if edge['type_'] not in EDGE_TYPE:\n",
    "        gg.delete_edges(edge)\n",
    "        pass\n",
    "for vertice in gg.vs:\n",
    "    if vertice.degree() == 0:\n",
    "        gg.delete_vertices(vertice)\n",
    "for vertice in gg.vs:\n",
    "    if vertice.degree() == 0:\n",
    "        gg.delete_vertices(vertice)\n",
    "g = copy.deepcopy(gg)\n",
    "g.delete_edges(g.es)\n",
    "def adding_edge(e, g):\n",
    "    if e not in g.es:\n",
    "        g.add_edge(e.source_vertex, e.target_vertex, type_=e['type_'], properties=e['properties'])\n",
    "cnt = 0\n",
    "\n",
    "for vertice in gg.vs:\n",
    "    v_type = list(vertice['labels'])[0]\n",
    "    if v_type == \"reaction\":\n",
    "        for e in vertice.all_edges():\n",
    "            if e['type_'] in ['enzyme', 'substrate', 'product']:\n",
    "            # if e['type_'] in ['enzyme', 'substrate', 'product', 'in_pathway']:\n",
    "                adding_edge(e, g)\n",
    "            elif e.source_vertex[\"properties\"]['kid'] in all_available_kid + gene_ortholog:\n",
    "                adding_edge(e, g)\n",
    "    elif v_type == \"gene\":\n",
    "        for e in vertice.all_edges():\n",
    "            if e['type_'] in ['gene_expression']:\n",
    "                adding_edge(e, g)\n",
    "\n",
    "edge_clear = True\n",
    "# for i in range(2):\n",
    "#     for vertice in g.vs:\n",
    "#         v_type = list(vertice['labels'])[0]\n",
    "#         if vertice.degree() <= 2 and v_type == \"reaction\":\n",
    "#             for e in vertice.all_edges():\n",
    "#                 g.delete_edges(e)\n",
    "\n",
    "# for vertice in g.vs:\n",
    "#         v_type = list(vertice['labels'])[0]\n",
    "#         if v_type == \"compound\":\n",
    "#             if vertice[\"properties\"]['kid'] not in all_available_kid + gene_ortholog:\n",
    "#                 try:\n",
    "#                     for e in vertice.all_edges():\n",
    "#                         g.delete_edges(e)      \n",
    "#                 except:\n",
    "#                     pass\n",
    "#                 g.delete_vertices(vertice)\n",
    "# while True:\n",
    "#     edge_clear = True\n",
    "#     for vertice in g.vs:\n",
    "#         v_type = list(vertice['labels'])[0]\n",
    "#         if vertice.degree() == 0 and v_type == \"reaction\":\n",
    "#             edge_clear = False\n",
    "#             g.delete_vertices(vertice)\n",
    "#         elif vertice.degree() == 0 and v_type in [\"ortholog\",\"gene\",'compound','module']:\n",
    "#             if vertice[\"properties\"]['kid'] not in all_available_kid and vertice[\"properties\"]['kid'] not in gene_ortholog:\n",
    "#                 edge_clear = False\n",
    "#                 g.delete_vertices(vertice)\n",
    "#             else:\n",
    "#                 edge_clear = False\n",
    "#                 g.delete_vertices(vertice)\n",
    "#     if edge_clear:\n",
    "#         break\n",
    "print(\"Final graph constructed\")\n",
    "end_time = time.time()\n",
    "print(str(round((end_time-start_time)/60, 2))+ \"min\")\n",
    "adj = g.get_adjacency()\n",
    "SIDE = adj.shape[0]\n",
    "avail = [True for i in range(SIDE)]\n",
    "cnt = 0\n",
    "comm_cnt = 0\n",
    "community_list = []\n",
    "community_list_sketch = []\n",
    "import json\n",
    "print(\"All done\")\n",
    "end_time = time.time()#'subGraph_light_'+str(THRESHOLD)+'_.graphml'\n",
    "print(str(round((end_time-start_time)/60, 2))+ \"min\")\n",
    "visual_style = {\n",
    "# \"vertex_size\" : 5,\n",
    "\"bbox\": (1000,1000),\n",
    "\"margin\": 60,\n",
    "\"layout\": \"kk\",\n",
    "# \"arrow_size\": 1,\n",
    "\"edge_width\": 0.3\n",
    "}\n",
    "node_labels_color = []\n",
    "for i in g.vs[\"properties\"]:\n",
    "    try:\n",
    "        y = score_dict[i['kid']]\n",
    "        if y < 1.0:#weight=3.0, gain=1.0\n",
    "            x = \"black\"\n",
    "        elif y < 2.0:#weight=3.0, gain=2.0\n",
    "            x = \"green\"\n",
    "        else:\n",
    "            x = \"red\"\n",
    "    except:\n",
    "        x = \"grey\"\n",
    "    node_labels_color.append(x)\n",
    "g.vs[\"label_color\"] = node_labels_color\n",
    "label_name = []\n",
    "for i in g.vs[\"properties\"]:\n",
    "    if i['kid'] in all_available_kid:\n",
    "        label_name.append(i['kid'] + \"=\" +str(round(score_dict[i['kid']], 2)))\n",
    "        # label_name.append(name_dict_reverse[i['kid']] + \"=\" +str(round(score_dict[i['kid']], 2)))\n",
    "    else:\n",
    "        label_name.append(i['kid'])\n",
    "g.vs[\"label\"] = label_name#['_nx_name':<id>, 'labels':类别, 'properties', 'label']\n",
    "col = {\"compound\":\"orange\", \"reaction\":\"pink\", \"gene\":\"brown\", \"ortholog\":\"green\", \"module\":\"blue\", \"pathway\": \"red\"}\n",
    "siz = {\"compound\":15, \"reaction\":5, \"gene\":15, \"ortholog\":15, \"module\":15, \"pathway\":15}\n",
    "g.vs[\"color\"] = [col[list(i)[0]] for i in g.vs[\"labels\"]]\n",
    "g.vs[\"size\"] = [siz[list(i)[0]] for i in g.vs[\"labels\"]]\n",
    "g.vs[\"label_dist\"] = [1 for i in g.vs[\"labels\"]]\n",
    "g.vs[\"label_size\"] = [6 for i in g.vs[\"labels\"]]\n",
    "# g.vs[\"label_color\"] = [\"blue\" for i in g.vs[\"labels\"]]\n",
    "#[i['kid'] for i in g.vs[\"properties\"]], [i['name'] for i in g.vs[\"properties\"]]\n",
    "#[list(i)[0] for i in g.vs[\"labels\"]]\n",
    "g.es['arrow_size'] = [0.5 for i in g.es['type_']]\n",
    "e_col = {'expression':\"green\", 'gene_expression':\"green\", 'repression':\"red\", \n",
    "'activation':\"green\", 'inhibition':\"red\", 'substrate':\"blue\", 'product':\"magenta\", 'enzyme':\"pink\", \"in_module\":\"orange\", \"in_pathway\":\"black\"}\n",
    "g.es['color'] = [e_col[i] for i in g.es['type_']]\n",
    "g.es['label'] = [i for i in g.es['type_']]\n",
    "g.es['label_color'] = [e_col[i] for i in g.es['type_']]\n",
    "g.es[\"label_size\"] = [6 for i in g.es[\"type_\"]]\n",
    "\n",
    "igraph.save(g, './TNBC_new/subGraph_light_2_'+str(THRESHOLD)+'_.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_imp = []\n",
    "for vertice in g.vs:\n",
    "  v_type = list(vertice['labels'])[0]\n",
    "  if v_type in \"reaction\":\n",
    "    num_nodes = 0\n",
    "    importance_value = 0\n",
    "    for e in vertice.all_edges():\n",
    "      v_mid = e.source_vertex\n",
    "      if list(v_mid['labels'])[0] == \"compound\":\n",
    "        try:\n",
    "          importance_value += score_dict[v_mid[\"properties\"]['kid']]\n",
    "          num_nodes += 1\n",
    "        except:\n",
    "          pass\n",
    "      elif list(v_mid['labels'])[0] == \"ortholog\":\n",
    "        for ed in v_mid.all_edges():\n",
    "          v_end = ed.source_vertex\n",
    "          if list(v_end['labels'])[0] == \"gene\":\n",
    "            importance_value += score_dict[v_end[\"properties\"]['kid']]\n",
    "            num_nodes += 1\n",
    "    if num_nodes == 0:\n",
    "      num_nodes += 1\n",
    "    li_imp.append([vertice[\"properties\"]['kid'], num_nodes, importance_value, importance_value / num_nodes])\n",
    "import math\n",
    "\n",
    "li_imp.sort(key= lambda x: (x[3]+math.log(1),x[1]), reverse=True)\n",
    "for i in li_imp:\n",
    "  print(i) \n",
    "print([li_imp[i][0] for i in range(20)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.TreeExplainer(model)\n",
    "data = train_xy#train, train_xy, val, tests\n",
    "cols = list(name_list)\n",
    "shap_values = explainer.shap_values(data[cols])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprehesional analysis of shap values and display of the list of importance.\n",
    "TYPE_NUMBER: {'BLIS':0, 'IM':1, 'LAR':2, 'MES':3}\n",
    "\n",
    "ANALYSIS_TYPE : {0:absolute value of shap_values, 1: original shap_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_0_list = []\n",
    "TYPE_NUMBER = 0\n",
    "ANALYSIS_TYPE = 0\n",
    "for i in range(len(name_list)):\n",
    "  if ANALYSIS_TYPE == 0:\n",
    "    imp_0_list.append(\n",
    "      [i, np.abs(shap_values[TYPE_NUMBER][:, i]).mean(), name_list[i]])\n",
    "  elif ANALYSIS_TYPE == 1:\n",
    "    imp_0_list.append([i, (shap_values[0][:,i]).mean(), name_list[i]])\n",
    "  else:\n",
    "    raise ValueError\n",
    "imp_0_list.sort(key= lambda x: x[1], reverse=True)\n",
    "# print([imp_0_list[i][1:] for i in range(20)])\n",
    "\n",
    "# for i in range(500):\n",
    "#   if np.abs(shap_values[0][:,i]).mean() > 0.0:\n",
    "#     print(i, np.abs(shap_values[0][:,i]).mean(), name_list[i])\n",
    "name_dict_reverse = {}\n",
    "for item in name_dict.keys():\n",
    "  name_dict_reverse[name_dict[item]] = item\n",
    "shap_0_dict = {}\n",
    "for item in imp_0_list:\n",
    "  shap_0_dict[item[2]] = item[1]\n",
    "li_imp = []\n",
    "for vertice in g.vs:\n",
    "  v_type = list(vertice['labels'])[0]\n",
    "  if v_type in \"reaction\":\n",
    "    num_nodes = 0\n",
    "    importance_value = 0\n",
    "    node_name = []\n",
    "    node_imp = []\n",
    "    for e in vertice.all_edges():\n",
    "      v_mid = e.source_vertex\n",
    "      if list(v_mid['labels'])[0] == \"compound\":\n",
    "        try:\n",
    "          importance_value += shap_0_dict[name_dict_reverse[v_mid[\"properties\"]['kid']]]\n",
    "          num_nodes += 1\n",
    "          node_name.append(name_dict_reverse[v_mid[\"properties\"]['kid']])\n",
    "          node_imp.append(shap_0_dict[name_dict_reverse[v_mid[\"properties\"]['kid']]])\n",
    "        except:\n",
    "          pass\n",
    "      elif list(v_mid['labels'])[0] == \"ortholog\":\n",
    "        for ed in v_mid.all_edges():\n",
    "          v_end = ed.source_vertex\n",
    "          if list(v_end['labels'])[0] == \"gene\":\n",
    "            try:\n",
    "              importance_value += shap_0_dict[name_dict_reverse[v_end[\"properties\"]['kid']]]\n",
    "              num_nodes += 1\n",
    "              node_name.append(name_dict_reverse[v_end[\"properties\"]['kid']])\n",
    "              node_imp.append(shap_0_dict[name_dict_reverse[v_end[\"properties\"]['kid']]])\n",
    "            except:\n",
    "              pass\n",
    "    if num_nodes == 0:\n",
    "      num_nodes += 1\n",
    "    dat = neo4j.run(\"match (n:reaction)-[p]-(q:pathway) where n.kid in ['{}'] return q;\".format(vertice[\"properties\"]['kid']))\n",
    "    li_imp.append([[vertice[\"properties\"]['kid'], [nn['q']['kid'] for nn in list(dat)]],\n",
    "       num_nodes, importance_value, importance_value / num_nodes, node_name, node_imp])\n",
    "import math\n",
    "\n",
    "# li_imp.sort(key= lambda x: (x[3]+math.log(1),x[1]), reverse=True)\n",
    "li_imp.sort(key= lambda x: (x[3] * (x[1] > 2),x[1]), reverse=True)\n",
    "def non_zero(list):\n",
    "  num = 0\n",
    "  eps = 1e-5\n",
    "  for i in list:\n",
    "    if abs(i) > eps:\n",
    "      num += 1\n",
    "  return num\n",
    "# li_imp.sort(key= lambda x: (x[3] * (x[1] > 2) * (non_zero(x[5]) > 2),x[1]), reverse=True)\n",
    "\n",
    "pathlist = []\n",
    "path_dict = {}\n",
    "over_pathlist = []\n",
    "for item in li_imp:\n",
    "  for name in item[0][1]:\n",
    "    if pathlist.count(name) == 0:\n",
    "      pathlist.append(name)\n",
    "      path_dict[name] = [item[0][0]]\n",
    "    else:\n",
    "      path_dict[name].append(item[0][0])\n",
    "\n",
    "for item in pathlist:\n",
    "  dat = neo4j.run(\"match (n:gene)-[p]-(q:pathway) where q.kid in ['{}'] return n;\".format(item))\n",
    "  if len(list(dat)) > 400:\n",
    "    over_pathlist.append(item)\n",
    "\n",
    "global_over_path = ['path:hsa01100','path:hsa01110','path:hsa01120','path:hsa01210',\n",
    "'path:hsa01212','path:hsa01230','path:hsa01232','path:hsa01250','path:hsa01240','path:hsa01220']\n",
    "for i in global_over_path:\n",
    "  if i not in over_pathlist:\n",
    "    over_pathlist.append(i)  \n",
    "\n",
    "for item in li_imp:\n",
    "  left = []\n",
    "  for name in item[0][1]:\n",
    "    if name not in over_pathlist:\n",
    "      left.append(name)\n",
    "  item[0][1] = left\n",
    "\n",
    "for i in li_imp:\n",
    "  print(i) \n",
    "# print([li_imp[i][0] for i in range(20)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Summary plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, data[cols])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('lyf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02764490cb7204177a8775c36778956e5230d457358b520b58b9ffbe46612377"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
